{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reinforcement-learning-pacman.ipynb",
      "provenance": [],
      "mount_file_id": "1QdkpjfI1_XMIasT3QvnHDT6f-BnjTHsz",
      "authorship_tag": "ABX9TyMDY3Ew79mutGsT/N+/K1ah",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madelinefuentes/reinforcement-learning-pacman/blob/master/reinforcement_learning_pacman.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvYPe-oPP2ze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxVmEl5uSdtN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym-retro"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cywfT-geSyW9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m retro.import '/gdrive/My Drive/Pacman'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwfZtSfaSA-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import retro\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.color import rgb2gray\n",
        "from skimage import transform\n",
        "from collections import deque\n",
        "import random\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import History\n",
        "\n",
        "\n",
        "def train():\n",
        "    env = retro.make(game = \"PacManNamco-Nes\")\n",
        "    obs = env.reset()\n",
        "\n",
        "    possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
        "\n",
        "    # model hyperparameters\n",
        "    state_size = [125, 80, 4]\n",
        "    learning_rate =  0.00025\n",
        "    action_size = env.action_space.n\n",
        "\n",
        "    #training hyperparameters\n",
        "    total_episodes = 1\n",
        "    max_steps = 50000\n",
        "    batch_size = 32\n",
        "    memory_size = 1000000\n",
        "\n",
        "    max_tau = 10000\n",
        "    max_exploration = 1.0\n",
        "    min_exploration = .01\n",
        "    decay_rate = 0.00001\n",
        "    discount = .9\n",
        "\n",
        "    memory = pretrain(memory_size, batch_size, env, possible_actions)\n",
        "    model = get_model(state_size, action_size, learning_rate)\n",
        "    target_model = get_model(state_size, action_size, learning_rate)\n",
        "    target_model = update_target_model(model, target_model)\n",
        "\n",
        "    history = History()\n",
        "    render_episode = True\n",
        "    decay_step = 0\n",
        "    tau = 0\n",
        "\n",
        "    for episode in range(total_episodes):\n",
        "        step = 0\n",
        "        episode_rewards = []\n",
        "\n",
        "        stacked_frames = add_frame(None, obs, True)\n",
        "        state = get_stacked_state(stacked_frames)\n",
        "\n",
        "        while step < max_steps:\n",
        "            step +=1 \n",
        "            decay_step +=1\n",
        "            action, explore_probability = predict_action(model, min_exploration, max_exploration,\n",
        "                decay_rate, decay_step, state, possible_actions)\n",
        "\n",
        "            obs, reward, done, info = env.step(action)\n",
        "            episode_rewards.append(reward)\n",
        "            \n",
        "            # if(render_episode):\n",
        "            #     env.render()\n",
        "            \n",
        "            # start next episode\n",
        "            if done:\n",
        "                terminal_frame = np.zeros((125,80), dtype=np.int)\n",
        "                terminal_frames = add_frame(stacked_frames, terminal_frame, False)\n",
        "                terminal_state = get_stacked_state(terminal_frames)\n",
        "                total_reward = np.sum(episode_rewards)\n",
        "                memory.add((state, action, reward, terminal_state, done))\n",
        "                step = max_steps\n",
        "                history_length = len(history.history['accuracy']) \n",
        "                accuracy = history.history['acuracy'][history_length - 1]\n",
        "                print('Episode: ' + str(episode + 1) + ' Total Reward: ' + str(total_reward) + ' Accuracy: ' + str(accuracy))\n",
        "\n",
        "            else:\n",
        "                stacked_frames = add_frame(stacked_frames, obs, False)\n",
        "                next_state = get_stacked_state(stacked_frames)\n",
        "                memory.add((state, action, reward, next_state, done))\n",
        "                state = next_state\n",
        "\n",
        "            batch = memory.sample(batch_size)\n",
        "            states_mb = np.array([each[0] for each in batch], ndmin=3)\n",
        "            actions_mb = np.array([each[1] for each in batch])\n",
        "            rewards_mb = np.array([each[2] for each in batch]) \n",
        "            next_states_mb = np.array([each[3] for each in batch], ndmin=3)\n",
        "            dones_mb = np.array([each[4] for each in batch])\n",
        "\n",
        "            next_q_values = target_model.predict(next_states_mb)\n",
        "            next_q_targets = model.predict(next_states_mb)\n",
        "            target_q_batch = []\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                terminal = dones_mb[i]\n",
        "                action = np.argmax(next_q_values[i])\n",
        "\n",
        "                if terminal:\n",
        "                    target_q_batch.append(rewards_mb[i])\n",
        "                        \n",
        "                else:\n",
        "                    target = rewards_mb[i] + discount * next_q_targets[i][action]\n",
        "                    target_q_batch.append(target)\n",
        "                        \n",
        "            targets_mb = np.array(target_q_batch)\n",
        "            model.fit(states_mb, targets_mb, verbose = 0, callbacks = [history])\n",
        "\n",
        "            if(tau > max_tau):\n",
        "                update_target_model(model, target_model)\n",
        "                tau = 0\n",
        "    \n",
        "        print('model saved')\n",
        "        model.save('after ' + str(episode) + ' episodes')\n",
        "\n",
        "\n",
        "# update target model's weights with prediction model's weight\n",
        "def update_target_model(model, target_model):\n",
        "    weights = model.get_weights()\n",
        "    target_model.set_weights(weights)\n",
        "    return target_model\n",
        "\n",
        "\n",
        "# use exploration vs exploitation tradeoff to decide how agent acts\n",
        "def predict_action(model, min_exploration, max_exploration,\n",
        "    decay_rate, decay_step, state, possible_actions):\n",
        "\n",
        "    tradeoff = np.random.rand()\n",
        "    explore_probability = min_exploration + (max_exploration - min_exploration) * np.exp(-decay_rate * decay_step)\n",
        "\n",
        "    if (explore_probability > tradeoff):\n",
        "        choice = np.random.choice(len(possible_actions))\n",
        "    else:\n",
        "        state_tensor = tf.convert_to_tensor(state)\n",
        "        state_tensor = tf.expand_dims(state_tensor, 0)\n",
        "        action_probs = model(state_tensor, training=False)\n",
        "        choice = np.argmax(action_probs)\n",
        "    \n",
        "    action = possible_actions[choice]\n",
        "    return action, explore_probability\n",
        "\n",
        "\n",
        "# initially fill memory with experience\n",
        "def pretrain(memory_size, pretrain_length, env, possible_actions):\n",
        "    memory = ReplayMemory(memory_size)\n",
        "    stacked_frames = []\n",
        "\n",
        "    for i in range(pretrain_length):\n",
        "        if i == 0:\n",
        "            obs = env.reset()\n",
        "            stacked_frames = add_frame(None, obs, True)\n",
        "\n",
        "        choice = np.random.choice(len(possible_actions))\n",
        "        action = possible_actions[choice]\n",
        "        obs, rew, done, info = env.step(action)\n",
        "        stacked_frames = add_frame(stacked_frames, obs, False)\n",
        "        state = get_stacked_state(stacked_frames)\n",
        "\n",
        "        if done:\n",
        "            next_state = np.zeros((1, 125, 80, 4))\n",
        "            memory.add((state, action, rew, next_state, done))\n",
        "            obs = env.reset()\n",
        "            stacked_frames = add_frame(None, obs, True)\n",
        "        else: \n",
        "            next_state = get_stacked_state(stacked_frames)\n",
        "            memory.add((state, action, rew, next_state, done))\n",
        "\n",
        "    return memory\n",
        "\n",
        "\n",
        "# format stacked frames for input to model\n",
        "def get_stacked_state(frames):\n",
        "    np_frames = np.stack(frames, axis=2)\n",
        "    return np_frames.reshape(1, 125, 80, 4)\n",
        "\n",
        "\n",
        "# convert frame to grayscale, crop, and normalize pixel values\n",
        "def preprocess_frame(frame):\n",
        "    gray_frame = rgb2gray(frame)\n",
        "    cropped = gray_frame[8:-4,0:-75]\n",
        "    normalized = cropped/255.0\n",
        "    resized = transform.resize(normalized, [125,80])\n",
        "    # plt.imshow(resized)\n",
        "    # plt.show()\n",
        "    return resized\n",
        "\n",
        "\n",
        "# add new state to stacked frames or initialize\n",
        "def add_frame(stacked_frames, new_state, is_new_episode):\n",
        "    frame = preprocess_frame(new_state)\n",
        "\n",
        "    # reinitialize when starting new episode\n",
        "    if(is_new_episode):\n",
        "        stacked_frames = deque([np.zeros((165,120), dtype=np.int) for i in range(4)], maxlen=4)\n",
        "        for n in range(4):\n",
        "            stacked_frames.append(frame)\n",
        "    else:\n",
        "        stacked_frames.append(frame)\n",
        "         \n",
        "    return stacked_frames\n",
        "\n",
        "\n",
        "# build convolutional neural network\n",
        "def get_model(state_size, action_size, learning_rate):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    model.add(layers.Conv2D(32, 8, strides = 4, activation='relu', input_shape = state_size))\n",
        "    model.add(layers.Conv2D(64, 4, strides = 2, activation='relu'))\n",
        "    model.add(layers.Conv2D(64, 4, strides = 1, activation='relu'))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(256, activation='relu'))\n",
        "    model.add(layers.Dense(action_size))\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate = learning_rate),\n",
        "        loss=tf.keras.losses.mean_squared_error,\n",
        "        metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# memory class for storing and sampling experiences\n",
        "class ReplayMemory:\n",
        "    def __init__(self, max_size):\n",
        "        self.buffer = deque(maxlen = max_size)\n",
        "\n",
        "    def add(self, experience_tuple):\n",
        "        self.buffer.append(experience_tuple)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        buffer_size = len(self.buffer)\n",
        "        indices = random.sample(range(buffer_size), batch_size)\n",
        "        return [self.buffer[index] for index in indices]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}